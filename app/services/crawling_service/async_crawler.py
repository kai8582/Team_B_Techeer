import aiohttp
import asyncio
from typing import Dict, List
import time
from datetime import datetime
from app.services.article_service.save import save_articles_batch
from app.services.crawling_service.rss_fetcher import fetch_rss_feed_async
from app.core.database import SessionLocal
from app.services.crawling_service.article_processor import process_article_with_summary

RSS_FEEDS = {
    "í•œêµ­ê²½ì œ": {
    #     # "ì „ì²´ë‰´ìŠ¤": "https://www.hankyung.com/feed/all-news",
        # "ì¦ê¶Œ": "https://www.hankyung.com/feed/finance",
    #     # "ê²½ì œ": "https://www.hankyung.com/feed/economy",
    #     # "ë¶€ë™ì‚°": "https://www.hankyung.com/feed/realestate",
         "IT": "https://www.hankyung.com/feed/it",
    #     # "ì •ì¹˜": "https://www.hankyung.com/feed/politics",
         "êµ­ì œ": "https://www.hankyung.com/feed/international",
        #  "ì‚¬íšŒ": "https://www.hankyung.com/feed/society",
        #   "ë¬¸í™”": "https://www.hankyung.com/feed/life",
        #  "ìŠ¤í¬ì¸ ": "https://www.hankyung.com/feed/sports",
    #     # "ì—°ì˜ˆ": "https://www.hankyung.com/feed/entertainment"
    },
    # "SBSë‰´ìŠ¤": {
        # "ì´ì‹œê° ì´ìŠˆ": "https://news.sbs.co.kr/news/headlineRssFeed.do?plink=RSSREADER",
        # "ìµœì‹ ": "https://news.sbs.co.kr/news/newsflashRssFeed.do?plink=RSSREADER",
        # "ì •ì¹˜": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01&plink=RSSREADER",
        # "ê²½ì œ": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=02&plink=RSSREADER",
        # "ì‚¬íšŒ": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=03&plink=RSSREADER",
        # "êµ­ì œ": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=07&plink=RSSREADER",
        # "ë¬¸í™”": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=08&plink=RSSREADER",
        # "ì—°ì˜ˆ": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=14&plink=RSSREADER",
        # "ìŠ¤í¬ì¸ ": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=09&plink=RSSREADER"
    # },
    # "ë§¤ì¼ê²½ì œ":{
    # # #     # "ì „ì²´ë‰´ìŠ¤":"https://www.mk.co.kr/rss/40300001",
    # # #     "ê²½ì œ":"https://www.mk.co.kr/rss/30100041",
    #     #  "ì •ì¹˜":"https://www.mk.co.kr/rss/30200030",
    # #     # "ì‚¬íšŒ":"https://www.mk.co.kr/rss/50400012",
    # #     "êµ­ì œ":"https://www.mk.co.kr/rss/30300018",
    # # #     "ì¦ê¶Œ":"https://www.mk.co.kr/rss/50200011",
    #      "ë¶€ë™ì‚°":"https://www.mk.co.kr/rss/50300009",
    # # #     "ë¬¸í™”":"https://www.mk.co.kr/rss/30000023",
    # # #     "ìŠ¤í¬ì¸ ":"https://www.mk.co.kr/rss/71000001",
    # #     "IT":"https://www.mk.co.kr/rssã„´/50700001"
    # },
}

"""ì¹´í…Œê³ ë¦¬ë³„ ë¹„ë™ê¸° í¬ë¡¤ë§"""
async def scrape_category_async(session: aiohttp.ClientSession, category: str, rss_url: str, 
                               press: str, semaphore: asyncio.Semaphore) -> List[Dict]:
    print(f"ğŸ“° {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...")
    category_start_time = time.time()
    article_urls = await fetch_rss_feed_async(session, rss_url) # RSS í”¼ë“œì—ì„œ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    if not article_urls:
        print(f"   âŒ {category} ì¹´í…Œê³ ë¦¬ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    print(f"   ğŸ“Š {category} ì¹´í…Œê³ ë¦¬: {len(article_urls)}ê°œ ê¸°ì‚¬ ë°œê²¬")
    
    # ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
    # í˜¹ì‹œë¼ë„ ë„ˆë¬´ ë§ì€ ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ ì„œë²„ ë¶€í•˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©
    async def process_with_semaphore(url, index):
        async with semaphore:
            # ìš”ì²­ ê°„ ì§€ì—° ì¶”ê°€ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            await asyncio.sleep(0.5)
            return await process_article_with_summary(session, url, category, press, index, len(article_urls))
    
    # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬
    tasks = [
        process_with_semaphore(url, i + 1) 
        for i, url in enumerate(article_urls)
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
    successful_articles: List[Dict] = []
    for result in results:
        if result is not None and not isinstance(result, Exception) and isinstance(result, dict):
            successful_articles.append(result)
    
    category_time = time.time() - category_start_time
    print(f"   ğŸ¯ {category} ì¹´í…Œê³ ë¦¬ ì™„ë£Œ: {len(successful_articles)}ê°œ ì„±ê³µ (ì†Œìš”ì‹œê°„: {category_time:.1f}ì´ˆ)")
    print("   " + "=" * 70)
    
    return successful_articles

"""ë¹„ë™ê¸° í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
async def scrape_all_articles_async(max_concurrent: int = 10, save_to_db: bool = True):
    scraped_articles = []
    start_time = time.time()
    total_articles = 0
    processed_articles = 0
    failed_articles = 0
    
    print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"âš¡ ë™ì‹œ ì²˜ë¦¬ ìˆ˜: {max_concurrent}")
    print("=" * 80)
    
    # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •)
    semaphore = asyncio.Semaphore(min(max_concurrent, 10))
    
    # aiohttp ì„¸ì…˜ ìƒì„± (ë” ë³´ìˆ˜ì ì¸ ì„¤ì •)
    connector = aiohttp.TCPConnector(limit=5, limit_per_host=3)
    timeout = aiohttp.ClientTimeout(total=60)
    
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬
    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        category_tasks = []
        for company, categories in RSS_FEEDS.items():
            for category, rss_url in categories.items():
                task = scrape_category_async(session, category, rss_url, company, semaphore)
                category_tasks.append(task)
        
        category_results = await asyncio.gather(*category_tasks, return_exceptions=True)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        for result in category_results:
            if isinstance(result, Exception):
                print(f"âŒ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                failed_articles += 1
            elif isinstance(result, list):
                scraped_articles.extend(result)
                processed_articles += len(result)
    
    # ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ í†µê³„
    total_time = time.time() - start_time
    success_rate = (processed_articles / (processed_articles + failed_articles) * 100) if (processed_articles + failed_articles) > 0 else 0
    
    print("=" * 80)
    print(f"ğŸ‰ ë¹„ë™ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ì „ì²´ í†µê³„:")
    print(f"   â€¢ ì„±ê³µ: {processed_articles}ê°œ")
    print(f"   â€¢ ì‹¤íŒ¨: {failed_articles}ê°œ")
    print(f"   â€¢ ì„±ê³µë¥ : {success_rate:.1f}%")
    print(f"   â€¢ ì´ ì†Œìš”ì‹œê°„: {total_time:.1f}ì´ˆ ({total_time/60:.1f}ë¶„)")
    print(f"   â€¢ í‰ê·  ì²˜ë¦¬ì‹œê°„: {total_time/processed_articles:.1f}ì´ˆ/ê¸°ì‚¬" if processed_articles > 0 else "   â€¢ í‰ê·  ì²˜ë¦¬ì‹œê°„: ê³„ì‚° ë¶ˆê°€")
    print("=" * 80)
    
    # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    if scraped_articles and save_to_db:
        print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œì‘...")
        db = SessionLocal()
        try:
            save_result = save_articles_batch(db, scraped_articles)
            print(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²°ê³¼:")
            print(f"   â€¢ ì €ì¥ ì„±ê³µ: {save_result['saved']}ê°œ")
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
        finally:
            db.close()
    elif scraped_articles and not save_to_db:
        print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê±´ë„ˆëœ€ (save_to_db=False)")
    
    return scraped_articles

# def scrape_all_articles_sync(save_to_db: bool = True):
#     """ë™ê¸° ë²„ì „ ë˜í¼ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´)"""
#     return asyncio.run(scrape_all_articles_async(save_to_db=save_to_db))
